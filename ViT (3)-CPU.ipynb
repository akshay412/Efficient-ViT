{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pQpgIbE5J6uR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, AutoFeatureExtractor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ulldiV6WJ9Bx"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 8\n",
    "num_epochs = 4\n",
    "seed = 42\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Load CIFAR100 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYUoPJFCKN6w",
    "outputId": "06359494-8c87-48a9-bbac-56ba021a1f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(testset, batch_size=eval_batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load pre-trained ViT model\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=100)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DgFthfPwKS8X"
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvAIns_DJ5X7",
    "outputId": "1414ea19-1ac5-40b4-f742-5875bed697fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 3125/3125 [29:53<00:00,  1.74it/s]\n",
      "Evaluating: 100%|██████████| 1250/1250 [01:58<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Train Loss: 1.1488, Train Accuracy: 0.7275\n",
      "Eval Loss: 0.7587, Eval Accuracy: 0.7886\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 3125/3125 [29:54<00:00,  1.74it/s]\n",
      "Evaluating: 100%|██████████| 1250/1250 [01:59<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4\n",
      "Train Loss: 0.5075, Train Accuracy: 0.8518\n",
      "Eval Loss: 0.6755, Eval Accuracy: 0.8113\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 3125/3125 [30:04<00:00,  1.73it/s]\n",
      "Evaluating: 100%|██████████| 1250/1250 [01:59<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4\n",
      "Train Loss: 0.3714, Train Accuracy: 0.8891\n",
      "Eval Loss: 0.6988, Eval Accuracy: 0.8056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 3125/3125 [30:01<00:00,  1.73it/s]\n",
      "Evaluating: 100%|██████████| 1250/1250 [01:59<00:00, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4\n",
      "Train Loss: 0.2958, Train Accuracy: 0.9101\n",
      "Eval Loss: 0.8210, Eval Accuracy: 0.7794\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    eval_correct = 0\n",
    "    eval_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            eval_total += labels.size(0)\n",
    "            eval_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    eval_loss /= len(test_loader)\n",
    "    eval_accuracy = eval_correct / eval_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9A_1pfWKYU5"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"vit_cifar100_finetuned.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yx16XQJa-3b9",
    "outputId": "bfbfe597-3201-44b3-fccf-e9c0d2207913"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict = torch.load('vit_cifar100_finetuned.pth', weights_only=True)\n",
    "state_dict = torch.load('vit_cifar100_finetuned.pth', weights_only=True, map_location=torch.device('cpu'))\n",
    "\n",
    "# Load the state dict into your model\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bIljev34DyCU"
   },
   "outputs": [],
   "source": [
    "def eval_op(model):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    eval_correct_top1 = 0\n",
    "    eval_correct_top3 = 0\n",
    "    eval_total = 0\n",
    "\n",
    "    # Start timing\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # Get the top 1 and top 3 predictions\n",
    "            _, top1_predicted = outputs.max(1)\n",
    "            _, top3_predicted = outputs.topk(3, 1, largest=True, sorted=True)\n",
    "\n",
    "            # Calculate top-1 accuracy\n",
    "            eval_correct_top1 += top1_predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Expand labels to match the shape of top3_predicted\n",
    "            labels_expanded = labels.view(-1, 1).expand_as(top3_predicted)\n",
    "\n",
    "            # Check if the correct label is in the top 3 predictions\n",
    "            correct_top3 = top3_predicted.eq(labels_expanded).any(dim=1)\n",
    "\n",
    "            eval_total += labels.size(0)\n",
    "            eval_correct_top3 += correct_top3.sum().item()\n",
    "\n",
    "    # End timing\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate evaluation time\n",
    "    eval_time = end_time - start_time\n",
    "\n",
    "    eval_loss /= len(test_loader)\n",
    "    eval_accuracy_top1 = eval_correct_top1 / eval_total\n",
    "    eval_accuracy_top3 = eval_correct_top3 / eval_total\n",
    "\n",
    "    print(f\"Eval Loss: {eval_loss:.4f}\")\n",
    "    print(f\"Eval Top-1 Accuracy: {eval_accuracy_top1:.4f}\")\n",
    "    print(f\"Eval Top-3 Accuracy: {eval_accuracy_top3:.4f}\")\n",
    "    print(f\"Evaluation Time: {eval_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l51ddNJD-6mG"
   },
   "outputs": [],
   "source": [
    "# Function to count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rD-NNazui1f",
    "outputId": "5e81148e-1d54-4a18-b5ce-e6ae5d5bd32c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained ViT model\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=100)\n",
    "model.to(device)\n",
    "\n",
    "state_dict = torch.load('vit_cifar100_finetuned.pth', weights_only=True, map_location=torch.device('cpu'))\n",
    "\n",
    "# Load the state dict into your model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "student_model = ViTForImageClassification.from_pretrained(\"facebook/deit-tiny-patch16-224\")\n",
    "student_model.classifier = torch.nn.Linear(student_model.classifier.in_features, 100)\n",
    "student_model.to(device)\n",
    "\n",
    "state_dict = torch.load('distilled_deit_tiny.pth', weights_only=True, map_location=torch.device('cpu'))\n",
    "\n",
    "# Load the state dict into your model\n",
    "student_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0_qbcpNEFyM",
    "outputId": "e3110e3a-ea25-4ed9-b479-6f0847a12895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85875556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1250/1250 [04:07<00:00,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.8210\n",
      "Eval Top-1 Accuracy: 0.7794\n",
      "Eval Top-3 Accuracy: 0.9228\n",
      "Evaluation Time: 247.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(count_parameters(model))\n",
    "eval_op(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cr0vkn08bdqU",
    "outputId": "ab389e0c-0cd2-4d26-b213-87e8cbdb0b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head #0\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #1\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #2\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #3\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #4\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #5\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #6\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #7\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #8\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #9\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #10\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "Head #11\n",
      "[Before Pruning] Num Heads: 12, Head Dim: 64 =>\n",
      "[After Pruning] Num Heads: 12, Head Dim: 64\n",
      "\n",
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 690, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTSdpaAttention(\n",
      "            (attention): ViTSdpaSelfAttention(\n",
      "              (query): Linear(in_features=690, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=690, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=690, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=690, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=690, out_features=2764, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=2764, out_features=690, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((690,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((690,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((690,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=690, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch_pruning as tp\n",
    "from transformers.models.vit.modeling_vit import ViTSelfAttention\n",
    "import math\n",
    "\n",
    "# Define the new forward function for ViTSelfAttention\n",
    "def new_forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
    "    batch_size, seq_length, _ = hidden_states.shape\n",
    "\n",
    "    mixed_query_layer = self.query(hidden_states)\n",
    "    mixed_key_layer = self.key(hidden_states)\n",
    "    mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "    query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "    key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "    value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "    # Normalize the attention scores to probabilities.\n",
    "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    # This is actually dropping out entire tokens to attend to, which might\n",
    "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "    attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "    # Mask heads if we want to\n",
    "    if head_mask is not None:\n",
    "        attention_probs = attention_probs * head_mask\n",
    "\n",
    "    context_layer = torch.matmul(attention_probs, value_layer)\n",
    "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "    new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "    context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "    outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# model.eval().to(\"cuda\")\n",
    "\n",
    "# Prepare example inputs\n",
    "example_inputs = torch.randn(1, 3, 224, 224)#.to(\"cuda\")\n",
    "\n",
    "# Set up pruning configuration\n",
    "num_heads = {}\n",
    "ignored_layers = [model.classifier]\n",
    "\n",
    "# Replace the forward function and set up num_heads\n",
    "for m in model.modules():\n",
    "    if isinstance(m, ViTSelfAttention):\n",
    "        m.forward = new_forward.__get__(m, ViTSelfAttention)\n",
    "        num_heads[m.query] = m.num_attention_heads\n",
    "\n",
    "imp = tp.importance.GroupNormImportance(2)\n",
    "pruner = tp.pruner.MetaPruner(\n",
    "    model,\n",
    "    example_inputs,\n",
    "    iterative_steps=5,\n",
    "    global_pruning=False,\n",
    "    importance=imp,\n",
    "    ignored_layers=ignored_layers,\n",
    "    num_heads=num_heads,\n",
    "    prune_head_dims=False,\n",
    "    prune_num_heads=True,\n",
    "    head_pruning_ratio=(1.0/10.0),\n",
    "    round_to=2,\n",
    ")\n",
    "\n",
    "# Perform pruning\n",
    "for i, g in enumerate(pruner.step(interactive=True)):\n",
    "    g.prune()\n",
    "\n",
    "# Modify the attention head size and all head size after pruning\n",
    "head_id = 0\n",
    "for m in model.modules():\n",
    "    if isinstance(m, ViTSelfAttention):\n",
    "        print(f\"Head #{head_id}\")\n",
    "        print(f\"[Before Pruning] Num Heads: {m.num_attention_heads}, Head Dim: {m.attention_head_size} =>\")\n",
    "        m.num_attention_heads = pruner.num_heads[m.query]\n",
    "        m.attention_head_size = m.query.out_features // m.num_attention_heads\n",
    "        m.all_head_size = m.num_attention_heads * m.attention_head_size\n",
    "        print(f\"[After Pruning] Num Heads: {m.num_attention_heads}, Head Dim: {m.attention_head_size}\")\n",
    "        print()\n",
    "        head_id += 1\n",
    "\n",
    "# Print the modified model structure\n",
    "print(model)\n",
    "\n",
    "# Save the pruned model\n",
    "pruned_model_path = \"pruned_model.pth\"\n",
    "model.save_pretrained(pruned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NX0i-PZuzjVX",
    "outputId": "163086eb-14e7-441b-e86f-fcc13886bcbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72056206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1250/1250 [03:52<00:00,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 1.1247\n",
      "Eval Top-1 Accuracy: 0.7015\n",
      "Eval Top-3 Accuracy: 0.8695\n",
      "Evaluation Time: 232.08 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# step 5\n",
    "print(count_parameters(model))\n",
    "eval_op(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmdjDfJrfQ9V",
    "outputId": "1fd0cc98-87f3-402c-d622-6d8967fc36f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68951428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1250/1250 [04:43<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 1.2572\n",
      "Eval Top-1 Accuracy: 0.6726\n",
      "Eval Top-3 Accuracy: 0.8476\n",
      "Evaluation Time: 283.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#step 4\n",
    "print(count_parameters(model))\n",
    "eval_op(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auuSszf31vBL",
    "outputId": "5ae4383c-b6f6-4554-9f14-bf56a7d031f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63703268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1250/1250 [03:59<00:00,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 1.7575\n",
      "Eval Top-1 Accuracy: 0.5660\n",
      "Eval Top-3 Accuracy: 0.7551\n",
      "Evaluation Time: 239.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#step 3\n",
    "print(count_parameters(model))\n",
    "eval_op(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMt8ErUj2Nqk",
    "outputId": "28ce3d7a-1cd7-4a30-a9d6-38bfd8099b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53796772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1250/1250 [03:28<00:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 3.4750\n",
      "Eval Top-1 Accuracy: 0.2342\n",
      "Eval Top-3 Accuracy: 0.3854\n",
      "Evaluation Time: 208.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#step 2\n",
    "print(count_parameters(model))\n",
    "eval_op(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3rE3hv15Lt8",
    "outputId": "de5d4d30-bca2-4677-8c41-7d0376c58403"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "teacher_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=100)\n",
    "teacher_model.to(device)\n",
    "\n",
    "state_dict = torch.load('vit_cifar100_finetuned.pth', weights_only=True, map_location=torch.device('cpu'))\n",
    "\n",
    "# Load the state dict into your model\n",
    "teacher_model.load_state_dict(state_dict)\n",
    "student_model = ViTForImageClassification.from_pretrained(\"facebook/deit-tiny-patch16-224\")\n",
    "student_model.classifier = torch.nn.Linear(student_model.classifier.in_features, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2-9e-ii75-6"
   },
   "outputs": [],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set models to evaluation mode\n",
    "teacher_model.eval()\n",
    "student_model.train()\n",
    "\n",
    "# Load feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-75M95Dz7-QX"
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Distillation parameters\n",
    "temperature = 2.0\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMNWtghH8CFD"
   },
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, labels, temperature=2.0, alpha=0.5):\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    soft_loss = F.kl_div(\n",
    "        F.log_softmax(student_logits / temperature, dim=1),\n",
    "        F.softmax(teacher_logits / temperature, dim=1),\n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "    return alpha * hard_loss + (1 - alpha) * soft_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    eval_total = 0\n",
    "    eval_correct_top1 = 0\n",
    "    eval_correct_top3 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Top-1 Accuracy\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            eval_correct_top1 += (predicted == labels).sum().item()\n",
    "\n",
    "            # Top-3 Accuracy\n",
    "            _, top3_predicted = outputs.logits.topk(3, 1, largest=True, sorted=True)\n",
    "            labels_expanded = labels.view(-1, 1).expand_as(top3_predicted)\n",
    "            correct = top3_predicted.eq(labels_expanded).any(dim=1)\n",
    "            eval_correct_top3 += correct.sum().item()\n",
    "\n",
    "            eval_total += labels.size(0)\n",
    "\n",
    "    accuracy_top1 = eval_correct_top1 / eval_total\n",
    "    accuracy_top3 = eval_correct_top3 / eval_total\n",
    "\n",
    "    return accuracy_top1, accuracy_top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mefS83Jy8ItF",
    "outputId": "8bdb8d7e-685e-4dd2-df07-65d7108d5f31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 3125/3125 [12:56<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 1.9217, Test Accuracy Top-1: 0.7226, Top-3: 0.9002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 3125/3125 [12:56<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.9378, Test Accuracy Top-1: 0.7553, Top-3: 0.9142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 3125/3125 [12:55<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.6997, Test Accuracy Top-1: 0.7682, Top-3: 0.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 3125/3125 [12:57<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Loss: 0.5574, Test Accuracy Top-1: 0.7785, Top-3: 0.9256\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Get teacher predictions\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(images)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # Get student predictions\n",
    "        student_outputs = student_model(images)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits, labels, temperature, alpha)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    test_accuracy_top1, test_accuracy_top3 = evaluate(student_model, test_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "          f\"Test Accuracy Top-1: {test_accuracy_top1:.4f}, Top-3: {test_accuracy_top3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVRhIZdQK_1P"
   },
   "outputs": [],
   "source": [
    "torch.save(student_model.state_dict(), 'distilled_deit_tiny.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2JbjiujRjCV",
    "outputId": "5cd2b150-dc93-4989-a46c-9b69f729a15d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5543716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1250/1250 [01:18<00:00, 15.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.7914\n",
      "Eval Top-1 Accuracy: 0.7785\n",
      "Eval Top-3 Accuracy: 0.9256\n",
      "Evaluation Time: 78.65 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(count_parameters(student_model))\n",
    "eval_op(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=100, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=100)\n",
    "state_dict = torch.load('vit_cifar100_finetuned.pth', weights_only=True, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# Perform dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "        {torch.nn.Linear, torch.nn.Embedding, torch.nn.LayerNorm},  # Quantize only Linear layers\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_model.state_dict(), 'vit_cifar100_quantized.pth')\n",
    "\n",
    "# To use the quantized model\n",
    "quantized_model.to(device)\n",
    "quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.8766\n",
      "Eval Top-1 Accuracy: 0.7673\n",
      "Eval Top-3 Accuracy: 0.9150\n",
      "Evaluation Time: 228.86 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(count_parameters(quantized_model))\n",
    "eval_op(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnxruntime\n",
      "  Using cached onnxruntime-1.20.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: coloredlogs in /home/ap8235/.local/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/ap8235/.local/lib/python3.12/site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/ap8235/.local/lib/python3.12/site-packages (from onnxruntime) (2.1.3)\n",
      "Requirement already satisfied: packaging in /ext3/miniforge3/lib/python3.12/site-packages (from onnxruntime) (24.1)\n",
      "Requirement already satisfied: protobuf in /home/ap8235/.local/lib/python3.12/site-packages (from onnxruntime) (5.28.2)\n",
      "Requirement already satisfied: sympy in /home/ap8235/.local/lib/python3.12/site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ap8235/.local/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ap8235/.local/lib/python3.12/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Using cached onnxruntime-1.20.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "Installing collected packages: onnxruntime\n",
      "\u001b[33m  WARNING: The script onnxruntime_test is installed in '/home/ap8235/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed onnxruntime-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "J7RsI1YMmvm0"
   },
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 1. Export the PyTorch Model to ONNX\n",
    "def export_to_onnx():\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    model.to(\"cpu\")  # Export on CPU\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)  # Batch size 1, RGB, 224x224\n",
    "\n",
    "    # Export the model\n",
    "    onnx_path = \"vit_model.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=16,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch_size\"}}\n",
    "    )\n",
    "    print(f\"ONNX model exported to {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "# 2. Quantize the ONNX Model\n",
    "def quantize_model(onnx_path):\n",
    "    quantized_onnx_path = \"vit_model_quantized.onnx\"\n",
    "    quantize_dynamic(\n",
    "        model_input=onnx_path,\n",
    "        model_output=quantized_onnx_path,\n",
    "        weight_type=QuantType.QInt8,\n",
    "        nodes_to_exclude=[\"/vit/embeddings/patch_embeddings/projection/Conv\"]\n",
    "    )\n",
    "    print(f\"Quantized ONNX model saved to {quantized_onnx_path}\")\n",
    "    return quantized_onnx_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "M7NJkiF1rNSm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm\n",
    "\n",
    "def eval_onnx_model(ort_session, test_loader, criterion, device=\"cuda\"):\n",
    "    eval_loss = 0.0\n",
    "    eval_correct_top1 = 0\n",
    "    eval_correct_top3 = 0\n",
    "    eval_total = 0\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move data to GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Run ONNX model inference\n",
    "            ort_inputs = {ort_session.get_inputs()[0].name: images.cpu().numpy()}\n",
    "            outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "            logits = torch.tensor(outputs[0], device=device)  # Ensure logits are on the GPU\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # Get top-1 and top-3 predictions\n",
    "            top1_predicted = logits.argmax(dim=1)\n",
    "            top3_predicted = logits.topk(3, 1, largest=True, sorted=True).indices\n",
    "\n",
    "            eval_correct_top1 += (top1_predicted == labels).sum().item()\n",
    "\n",
    "            # Check if the correct label is in the top-3 predictions\n",
    "            correct_top3 = top3_predicted.eq(labels.view(-1, 1).expand_as(top3_predicted)).any(dim=1)\n",
    "            eval_correct_top3 += correct_top3.sum().item()\n",
    "\n",
    "            eval_total += labels.size(0)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    eval_time = end_time - start_time\n",
    "\n",
    "    eval_loss /= len(test_loader)\n",
    "    eval_accuracy_top1 = eval_correct_top1 / eval_total\n",
    "    eval_accuracy_top3 = eval_correct_top3 / eval_total\n",
    "\n",
    "    print(f\"Eval Loss: {eval_loss:.4f}\")\n",
    "    print(f\"Eval Top-1 Accuracy: {eval_accuracy_top1:.4f}\")\n",
    "    print(f\"Eval Top-3 Accuracy: {eval_accuracy_top3:.4f}\")\n",
    "    print(f\"Evaluation Time: {eval_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9r4FBR8m0GT",
    "outputId": "5a952eee-ef36-408e-863f-55752f919cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model exported to vit_model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Export PyTorch model to ONNX\n",
    "onnx_path = export_to_onnx()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duJwHRxFnb8e",
    "outputId": "82f5bfb6-c4a3-4903-c811-55e339bfbcba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized ONNX model saved to vit_model_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Quantize the ONNX model\n",
    "quantized_onnx_path = quantize_model(onnx_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRDUyV-nndEo",
    "outputId": "e3ebe2c2-3d06-4600-d9d3-323814c05c77"
   },
   "outputs": [
    {
     "ename": "NotImplemented",
     "evalue": "[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/vit/embeddings/patch_embeddings/projection/Conv_quant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplemented\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m providers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPUExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m ort_session \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_onnx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproviders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m eval_onnx_model(ort_session, test_loader, criterion, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:465\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:537\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    534\u001b[0m     disabled_optimizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess \u001b[38;5;241m=\u001b[39m sess\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess\u001b[38;5;241m.\u001b[39msession_options\n",
      "\u001b[0;31mNotImplemented\u001b[0m: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/vit/embeddings/patch_embeddings/projection/Conv_quant'"
     ]
    }
   ],
   "source": [
    "providers = [\"CPUExecutionProvider\"]\n",
    "ort_session = ort.InferenceSession(quantized_onnx_path, providers=providers)\n",
    "eval_onnx_model(ort_session, test_loader, criterion, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qQatE9wmi6q",
    "outputId": "9b43f8e2-a127-4abc-a15d-32e6b8de3fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Torch-Pruning'...\n",
      "remote: Enumerating objects: 6909, done.\u001b[K\n",
      "remote: Total 6909 (delta 0), reused 0 (delta 0), pack-reused 6909 (from 1)\u001b[K\n",
      "Receiving objects: 100% (6909/6909), 10.08 MiB | 13.50 MiB/s, done.\n",
      "Resolving deltas: 100% (4633/4633), done.\n",
      "Obtaining file:///content/Torch-Pruning\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-pruning==1.5.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-pruning==1.5.0) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-pruning==1.5.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-pruning==1.5.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch-pruning==1.5.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-pruning==1.5.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch-pruning==1.5.0) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torch-pruning==1.5.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torch-pruning==1.5.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-pruning==1.5.0) (3.0.2)\n",
      "Installing collected packages: torch-pruning\n",
      "  Running setup.py develop for torch-pruning\n",
      "Successfully installed torch-pruning-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/VainF/Torch-Pruning.git\n",
    "!cd Torch-Pruning && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IX89N7ivmbhA"
   },
   "outputs": [],
   "source": [
    "!cp -r Torch-Pruning/torch_pruning/ torch_pruning/\n",
    "!rm -rf Torch-Pruning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JghVr7_ML2sO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hpml",
   "language": "python",
   "name": "hpml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
